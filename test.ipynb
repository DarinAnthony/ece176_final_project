{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dqn.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Import your modules\n",
    "from utils.preprocessor import DQNPreprocessor\n",
    "from models.base import DQN\n",
    "from utils.replayBuffer import ReplayBuffer\n",
    "from agents.base import DQNAgent\n",
    "from utils.visualizer import DQNVisualizer  # The visualizer class you shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_validation(agent, env_name, max_frames=1000):\n",
    "    \"\"\"\n",
    "    Run a quick validation of the agent to check for obvious errors.\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    # Reset preprocessor\n",
    "    if hasattr(agent, 'preprocessor'):\n",
    "        agent.preprocessor.reset()\n",
    "    \n",
    "    state = agent.get_state(obs)\n",
    "    \n",
    "    frame_count = 0\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    # Track tensors for sanity checks\n",
    "    states_sample = []\n",
    "    \n",
    "    print(\"Starting quick validation...\")\n",
    "    \n",
    "    while not done and frame_count < max_frames:\n",
    "        # Select action\n",
    "        action = agent.select_action(state)\n",
    "        \n",
    "        # Execute action\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Process next state\n",
    "        next_state = agent.get_state(obs)\n",
    "        \n",
    "        # Store in replay buffer\n",
    "        agent.memory.add(state, action, np.sign(reward), next_state, done)\n",
    "        \n",
    "        # Store sample for checking\n",
    "        if len(states_sample) < 5:\n",
    "            states_sample.append(state.cpu().numpy())\n",
    "        \n",
    "        # Update\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        frame_count += 1\n",
    "        \n",
    "        # Run optimization steps\n",
    "        if frame_count % 10 == 0 and len(agent.memory) > agent.batch_size:\n",
    "            agent._optimize_model()\n",
    "            print(f\"Frame {frame_count}: Optimization step completed\")\n",
    "    \n",
    "    print(f\"Validation run completed: {frame_count} frames, reward: {total_reward}\")\n",
    "    \n",
    "    # Sanity checks\n",
    "    print(\"\\nRunning sanity checks:\")\n",
    "    \n",
    "    # Check state shapes\n",
    "    print(f\"State shape: {state.shape} (expected: [1, 4, 84, 84])\")\n",
    "    \n",
    "    # Check replay buffer\n",
    "    print(f\"Replay buffer size: {len(agent.memory)}\")\n",
    "    \n",
    "    # Check if states differ (not all identical)\n",
    "    if len(states_sample) > 1:\n",
    "        differences = np.mean(np.abs(states_sample[0] - states_sample[1]))\n",
    "        print(f\"Mean difference between states: {differences}\")\n",
    "        if differences < 0.01:\n",
    "            print(\"WARNING: States appear very similar, check preprocessing\")\n",
    "        else:\n",
    "            print(\"States appear to differ correctly\")\n",
    "    \n",
    "    # Check optimization\n",
    "    if len(agent.memory) > agent.batch_size:\n",
    "        states, actions, rewards, next_states, dones = agent.memory.sample(agent.batch_size)\n",
    "        print(f\"Sample batch shapes: states {states.shape}, actions {actions.shape}\")\n",
    "        \n",
    "        # Try forward pass\n",
    "        with torch.no_grad():\n",
    "            q_values = agent.policy_net(states)\n",
    "            print(f\"Q-values shape: {q_values.shape} (expected: [{agent.batch_size}, {agent.num_actions}])\")\n",
    "            print(f\"Q-values range: {q_values.min().item():.4f} to {q_values.max().item():.4f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_training(agent, env_name, num_frames=10000, eval_interval=2000):\n",
    "    \"\"\"\n",
    "    Run a mini training session to ensure all components work together.\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    progress_bar = tqdm(total=num_frames, desc=\"Mini training\")\n",
    "    \n",
    "    frame_count = 0\n",
    "    episode_count = 0\n",
    "    rewards_history = []\n",
    "    \n",
    "    while frame_count < num_frames:\n",
    "        # Reset environment\n",
    "        obs, _ = env.reset()\n",
    "        state = agent.get_state(obs)\n",
    "        \n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done and frame_count < num_frames:\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Skip frames (act every k frames)\n",
    "            total_reward = 0\n",
    "            for _ in range(agent.frame_skip):\n",
    "                obs, reward, terminated, truncated, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                done = terminated or truncated\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # Process new frame\n",
    "            next_state = agent.get_state(obs)\n",
    "            \n",
    "            # Clip rewards\n",
    "            clipped_reward = np.sign(total_reward)\n",
    "            \n",
    "            # Store transition\n",
    "            agent.memory.add(state, action, clipped_reward, next_state, done)\n",
    "            \n",
    "            # Update\n",
    "            state = next_state\n",
    "            episode_reward += total_reward\n",
    "            frame_count += 1\n",
    "            \n",
    "            # Optimize model\n",
    "            if len(agent.memory) > agent.batch_size:\n",
    "                agent._optimize_model()\n",
    "            \n",
    "            # Update target network\n",
    "            if frame_count % agent.target_update == 0:\n",
    "                agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        # Episode finished\n",
    "        episode_count += 1\n",
    "        rewards_history.append(episode_reward)\n",
    "        \n",
    "        # Print stats\n",
    "        print(f\"\\nEpisode {episode_count}, Frames: {frame_count}, Reward: {episode_reward:.2f}\")\n",
    "    \n",
    "    progress_bar.close()\n",
    "    print(\"Mini training completed!\")\n",
    "    \n",
    "    # Plot rewards\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards_history)\n",
    "    plt.title(\"Episode Rewards\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save the model\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    torch.save(agent.policy_net.state_dict(), f\"models/mini_train_{env_name.replace('/', '_')}.pth\")\n",
    "    \n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_agent(agent, env_name, num_episodes=2, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Create and run a visualizer to see the agent in action.\n",
    "    \"\"\"\n",
    "    # Create visualizer\n",
    "    visualizer = DQNVisualizer(\n",
    "        env_name=env_name,\n",
    "        agent=agent,\n",
    "        output_dir='./videos',\n",
    "        show_metrics=True,\n",
    "        show_q_values=True,\n",
    "        show_preprocessed=True\n",
    "    )\n",
    "    \n",
    "    # Add a Q-values method to the agent for visualization if it doesn't exist\n",
    "    if not hasattr(agent, 'get_q_values'):\n",
    "        def get_q_values(state):\n",
    "            # Convert numpy array to PyTorch tensor if it's not already a tensor\n",
    "            if isinstance(state, np.ndarray):\n",
    "                # Convert from HWC to CHW format and add batch dimension\n",
    "                state = np.transpose(state, (2, 0, 1))  # Convert from (84,84,4) to (4,84,84)\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                return agent.policy_net(state).cpu().numpy()[0]\n",
    "        agent.get_q_values = get_q_values\n",
    "    \n",
    "    # Add process_observation method if needed\n",
    "    if not hasattr(agent, 'process_observation'):\n",
    "        def process_observation(obs):\n",
    "            # Call get_state but extract the numpy array before tensor conversion\n",
    "            agent.preprocessor.reset()\n",
    "            state = None\n",
    "            while state is None:\n",
    "                state = agent.preprocessor.process(obs)\n",
    "                if state is None:\n",
    "                    # This should handle itself in the visualizer\n",
    "                    return None\n",
    "            return state\n",
    "        agent.process_observation = process_observation\n",
    "    \n",
    "    # Record episodes\n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"\\nRecording episode {episode+1}...\")\n",
    "        episode_stats = visualizer.record_episode(\n",
    "            filename=f\"{env_name.replace('/', '_')}_episode_{episode+1}\",\n",
    "            max_steps=max_steps,\n",
    "            render=True\n",
    "        )\n",
    "    \n",
    "    # Plot performance metrics\n",
    "    visualizer.plot_performance_metrics(\n",
    "        filename=f\"{env_name.replace('/', '_')}_performance\",\n",
    "        show=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dqn():\n",
    "    # Set up environment\n",
    "    env_name = 'ALE/Breakout-v5'\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    # Set up device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create agent\n",
    "    agent = DQNAgent(\n",
    "        env=env,\n",
    "        replayBufferClass=ReplayBuffer,\n",
    "        QNetwork=DQN,\n",
    "        PreprocessorClass=DQNPreprocessor,\n",
    "        device=device,\n",
    "        memory_size=10000,  # Smaller for testing\n",
    "        batch_size=32,\n",
    "        target_update=500   # Smaller for testing\n",
    "    )\n",
    "    \n",
    "    # Quick validation (catches basic errors)\n",
    "    print(\"\\n=== Running Quick Validation ===\")\n",
    "    quick_validation(agent, env_name, max_frames=500)\n",
    "    \n",
    "    # Mini training session (catches training-related issues)\n",
    "    print(\"\\n=== Running Mini Training ===\")\n",
    "    mini_training(agent, env_name, num_frames=5000)\n",
    "    \n",
    "    # Visualize the agent\n",
    "    print(\"\\n=== Visualizing Agent ===\")\n",
    "    visualize_agent(agent, env_name, num_episodes=1, max_steps=500)\n",
    "    \n",
    "    print(\"\\nAll tests completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dqn()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
