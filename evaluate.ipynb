{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "os.chdir('/content/drive/My Drive/projects/ece176_final_project')\n",
    "\n",
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Import your modules\n",
    "from utils.preprocessor import DQNPreprocessor\n",
    "from models.base import DQN\n",
    "from models.dqn2015 import DQN2\n",
    "from utils.visualizer import DQNVisualizer\n",
    "from agents.base import DQNAgent\n",
    "from utils.replayBuffer import ReplayBuffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_path, env_name, num_episodes=30, record_episodes=5, \n",
    "                  output_dir='./eval_runs', device=None,\n",
    "                  record_length=3000, comparison_data=None):\n",
    "    \"\"\"\n",
    "    Evaluate a trained DQN model on an Atari environment.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the trained model\n",
    "        env_name (str): Name of the Atari environment\n",
    "        num_episodes (int): Total number of evaluation episodes\n",
    "        record_episodes (int): Number of episodes to record videos for\n",
    "        output_dir (str): Directory to save evaluation results\n",
    "        device (str): Device to run evaluation on ('cuda' or 'cpu')\n",
    "        record_length (int): Maximum length of recorded episodes\n",
    "        comparison_data (dict): Optional baseline data for comparison\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results\n",
    "    \"\"\"\n",
    "    # Setup device\n",
    "    if device is None:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Setup output directory with timestamp\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(output_dir, f\"{env_name.replace('/', '_')}_{timestamp}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(run_dir, \"videos\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(run_dir, \"metrics\"), exist_ok=True)\n",
    "    \n",
    "    # Create environment\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    # Create agent\n",
    "    agent = DQNAgent(\n",
    "        env=env,\n",
    "        replayBufferClass=ReplayBuffer,\n",
    "        QNetwork=DQN2,  # Use the same network architecture as in training\n",
    "        PreprocessorClass=DQNPreprocessor,\n",
    "        device=device,\n",
    "        memory_size=10000,  # Doesn't matter for evaluation\n",
    "        batch_size=32,      # Doesn't matter for evaluation\n",
    "        target_update=500,  # Doesn't matter for evaluation\n",
    "        weights_path=model_path  # Load weights from path\n",
    "    )\n",
    "    \n",
    "    # Create visualizer\n",
    "    visualizer = DQNVisualizer(\n",
    "        env_name=env_name,\n",
    "        agent=agent,\n",
    "        output_dir=run_dir,\n",
    "        show_metrics=True,\n",
    "        show_q_values=True,\n",
    "        show_preprocessed=True\n",
    "    )\n",
    "    \n",
    "    # this part may be redundant since the agent __init__ already takes in the weights path\n",
    "    # # Load model checkpoint to extract metadata\n",
    "    # model_info = agent.load_model(model_path)\n",
    "    \n",
    "    # print(\"\\n=== Model Information ===\")\n",
    "    # for key, value in model_info.items():\n",
    "    #     print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(f\"\\n=== Starting Evaluation ({num_episodes} episodes) ===\")\n",
    "    \n",
    "    # Metrics storage\n",
    "    rewards = []\n",
    "    lengths = []\n",
    "    q_values = []\n",
    "    actions_taken = defaultdict(int)\n",
    "    \n",
    "    # 1. Run standard evaluation using agent's built-in method\n",
    "    print(\"\\nRunning standard evaluation...\")\n",
    "    standard_mean_reward = agent.evaluate(num_episodes=num_episodes)\n",
    "    print(f\"Standard evaluation complete. Mean reward: {standard_mean_reward:.2f}\")\n",
    "    \n",
    "    # 2. Record specific episodes for visualization\n",
    "    record_indices = random.sample(range(num_episodes), min(record_episodes, num_episodes))\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        is_recording = episode in record_indices\n",
    "        \n",
    "        if is_recording:\n",
    "            print(f\"\\nRecording episode {episode+1}/{num_episodes}...\")\n",
    "            # Use visualizer to record this episode\n",
    "            episode_stats = visualizer.record_episode(\n",
    "                filename=f\"episode_{episode+1}\",\n",
    "                max_steps=record_length,\n",
    "                render=True\n",
    "            )\n",
    "            # Get metrics from recording\n",
    "            rewards.append(episode_stats.get('reward', 0))\n",
    "            lengths.append(episode_stats.get('length', 0))\n",
    "            \n",
    "            # Count actions taken\n",
    "            for action in episode_stats['actions']:\n",
    "                actions_taken[int(action)] += 1\n",
    "            \n",
    "            # Collect Q-values if available\n",
    "            if episode_stats['q_values']:\n",
    "                q_values.extend(episode_stats['q_values'])\n",
    "        else:\n",
    "            print(f\"\\nRunning episode {episode+1}/{num_episodes}...\")\n",
    "            # Run episode without recording using agent's run_episode function\n",
    "            reward, length, episode_actions, episode_q_values = agent.run_episode(\n",
    "                max_steps=18000, \n",
    "                record=False,\n",
    "                evaluate=True\n",
    "            )\n",
    "            \n",
    "            # Store metrics\n",
    "            rewards.append(reward)\n",
    "            lengths.append(length)\n",
    "            if episode_q_values:\n",
    "                q_values.extend(episode_q_values)\n",
    "            \n",
    "            # Count actions\n",
    "            for action in episode_actions:\n",
    "                actions_taken[int(action)] += 1\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Episode {episode+1} - Reward: {rewards[-1]}, Length: {lengths[-1]}\")\n",
    "    \n",
    "    # Calculate aggregate metrics\n",
    "    mean_reward = np.mean(rewards)\n",
    "    median_reward = np.median(rewards)\n",
    "    std_reward = np.std(rewards)\n",
    "    max_reward = np.max(rewards)\n",
    "    \n",
    "    mean_length = np.mean(lengths)\n",
    "    median_length = np.median(lengths)\n",
    "    \n",
    "    # Calculate average max Q-value if we have q_values\n",
    "    mean_max_q = float(np.mean([np.max(q) for q in q_values])) if q_values else None\n",
    "    \n",
    "    # Generate comparison with DQN paper if provided\n",
    "    paper_comparison = compare_with_dqn_paper(env_name, mean_reward)\n",
    "    \n",
    "    # Compile all results\n",
    "    results = {\n",
    "        \"env_name\": env_name,\n",
    "        \"model_path\": model_path,\n",
    "        \"num_episodes\": num_episodes,\n",
    "        \"rewards\": {\n",
    "            \"mean\": float(mean_reward),\n",
    "            \"median\": float(median_reward),\n",
    "            \"std\": float(std_reward),\n",
    "            \"max\": float(max_reward),\n",
    "            \"all_rewards\": [float(r) for r in rewards]\n",
    "        },\n",
    "        \"lengths\": {\n",
    "            \"mean\": float(mean_length),\n",
    "            \"median\": float(median_length),\n",
    "            \"all_lengths\": [int(l) for l in lengths]\n",
    "        },\n",
    "        \"actions\": {\n",
    "            \"distribution\": {str(k): int(v) for k, v in actions_taken.items()}\n",
    "        },\n",
    "        # \"model_info\": model_info,\n",
    "        \"paper_comparison\": paper_comparison,\n",
    "        \"timestamp\": timestamp\n",
    "    }\n",
    "    \n",
    "    if mean_max_q is not None:\n",
    "        results[\"q_values\"] = {\n",
    "            \"mean_max_q\": mean_max_q\n",
    "        }\n",
    "    \n",
    "    \n",
    "    # Before saving to JSON\n",
    "    results = make_json_serializable(results)\n",
    "    \n",
    "    # Save results to JSON\n",
    "    results_path = os.path.join(run_dir, \"evaluation_results.json\")\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    # Generate plots\n",
    "    generate_plots(results, run_dir)\n",
    "    \n",
    "    print(\"\\n=== Evaluation Summary ===\")\n",
    "    print(f\"Environment: {env_name}\")\n",
    "    print(f\"Model: {Path(model_path).name}\")\n",
    "    print(f\"Episodes: {num_episodes}\")\n",
    "    print(f\"Mean reward: {mean_reward:.2f} Â± {std_reward:.2f}\")\n",
    "    print(f\"Median reward: {median_reward:.2f}\")\n",
    "    print(f\"Max reward: {max_reward:.2f}\")\n",
    "    \n",
    "    if paper_comparison:\n",
    "        print(\"\\n=== Comparison with DQN Paper ===\")\n",
    "        print(f\"Your agent: {mean_reward:.2f}\")\n",
    "        print(f\"DQN paper: {paper_comparison.get('dqn_paper_score', 'N/A')}\")\n",
    "        print(f\"Human normalized score: {paper_comparison.get('human_normalized_score', 'N/A'):.2f}\")\n",
    "        print(f\"% of DQN paper performance: {paper_comparison.get('percent_of_dqn_paper_performance', 'N/A'):.2f}%\")\n",
    "    \n",
    "    print(f\"\\nResults saved to: {run_dir}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_dqn_paper(env_name, mean_reward):\n",
    "    \"\"\"\n",
    "    Compare results with the DQN Nature paper.\n",
    "    \n",
    "    Args:\n",
    "        env_name (str): Environment name\n",
    "        mean_reward (float): Mean reward achieved\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comparison metrics\n",
    "    \"\"\"\n",
    "    # DQN paper results (mean scores from the Nature paper)\n",
    "    dqn_paper_results = {\n",
    "        'breakout': {'dqn_score': 401.2, 'human_score': 31.8, 'random_score': 1.7},\n",
    "        'pong': {'dqn_score': 20.9, 'human_score': 9.3, 'random_score': -20.7},\n",
    "        'space_invaders': {'dqn_score': 1976.0, 'human_score': 1652.0, 'random_score': 148.0},\n",
    "        'seaquest': {'dqn_score': 5286.0, 'human_score': 20182.0, 'random_score': 68.0},\n",
    "        'beam_rider': {'dqn_score': 6846.0, 'human_score': 5775.0, 'random_score': 363.9},\n",
    "        'enduro': {'dqn_score': 301.8, 'human_score': 309.6, 'random_score': 0.0},\n",
    "        'qbert': {'dqn_score': 10596.0, 'human_score': 13455.0, 'random_score': 157.5}\n",
    "    }\n",
    "    \n",
    "    # Extract game name\n",
    "    game_name = env_name.lower().split('/')[-1].split('-')[0]\n",
    "    \n",
    "    if game_name in dqn_paper_results:\n",
    "        paper_data = dqn_paper_results[game_name]\n",
    "        \n",
    "        # Calculate human-normalized score: (agent_score - random_score) / (human_score - random_score)\n",
    "        human_norm_score = (mean_reward - paper_data['random_score']) / (paper_data['human_score'] - paper_data['random_score'])\n",
    "        \n",
    "        # Calculate percentage of DQN paper performance\n",
    "        dqn_performance_pct = (mean_reward / paper_data['dqn_score']) * 100\n",
    "        \n",
    "        return {\n",
    "            'game': game_name,\n",
    "            'your_score': float(mean_reward),\n",
    "            'dqn_paper_score': paper_data['dqn_score'],\n",
    "            'human_score': paper_data['human_score'],\n",
    "            'random_score': paper_data['random_score'],\n",
    "            'human_normalized_score': float(human_norm_score),\n",
    "            'percent_of_dqn_paper_performance': float(dqn_performance_pct),\n",
    "            'exceeds_human': mean_reward > paper_data['human_score'],\n",
    "            'exceeds_dqn_paper': mean_reward > paper_data['dqn_score']\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'game': game_name,\n",
    "            'note': 'No DQN paper results available for comparison'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plots(results, output_dir):\n",
    "    \"\"\"\n",
    "    Generate and save evaluation plots.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Evaluation results\n",
    "        output_dir (str): Output directory\n",
    "    \"\"\"\n",
    "    # 1. Rewards plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results['rewards']['all_rewards'])\n",
    "    plt.axhline(y=results['rewards']['mean'], color='r', linestyle='--', \n",
    "                label=f'Mean: {results[\"rewards\"][\"mean\"]:.2f}')\n",
    "    plt.title(f\"Rewards for {results['env_name']}\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, \"metrics/rewards.png\"))\n",
    "    \n",
    "    # 2. Episode lengths plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results['lengths']['all_lengths'])\n",
    "    plt.axhline(y=results['lengths']['mean'], color='r', linestyle='--', \n",
    "                label=f'Mean: {results[\"lengths\"][\"mean\"]:.2f}')\n",
    "    plt.title(f\"Episode Lengths for {results['env_name']}\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Steps\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, \"metrics/episode_lengths.png\"))\n",
    "    \n",
    "    # 3. Action distribution\n",
    "    if 'distribution' in results['actions']:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        actions = [int(k) for k in results['actions']['distribution'].keys()]\n",
    "        counts = [int(v) for v in results['actions']['distribution'].values()]\n",
    "        plt.bar(actions, counts)\n",
    "        plt.title(f\"Action Distribution for {results['env_name']}\")\n",
    "        plt.xlabel(\"Action\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(actions)\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.savefig(os.path.join(output_dir, \"metrics/action_distribution.png\"))\n",
    "    \n",
    "    # 4. Comparison with DQN paper if available\n",
    "    if results['paper_comparison'] and 'dqn_paper_score' in results['paper_comparison']:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        scores = [\n",
    "            results['paper_comparison']['random_score'],\n",
    "            results['rewards']['mean'],\n",
    "            results['paper_comparison']['dqn_paper_score'],\n",
    "            results['paper_comparison']['human_score']\n",
    "        ]\n",
    "        labels = ['Random', 'Your Agent', 'DQN Paper', 'Human']\n",
    "        plt.bar(labels, scores)\n",
    "        plt.title(f\"Performance Comparison for {results['env_name']}\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.savefig(os.path.join(output_dir, \"metrics/comparison.png\"))\n",
    "    \n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json_serializable(obj):\n",
    "    \"\"\"Convert any object to JSON-compatible types.\"\"\"\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        return obj.cpu().detach().numpy().tolist()\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, bool):  # Boolean handling\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, (int, float, str, type(None))):\n",
    "        return obj  # These types are natively JSON serializable\n",
    "    elif isinstance(obj, dict):\n",
    "        return {str(k): make_json_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [make_json_serializable(item) for item in obj]\n",
    "    elif hasattr(obj, '__dict__'):  # Handle custom objects\n",
    "        return {k: make_json_serializable(v) for k, v in obj.__dict__.items()\n",
    "                if not k.startswith('_')}\n",
    "    else:\n",
    "        # Last resort: convert to string\n",
    "        return str(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "evaluate_model(\n",
    "    model_path=\"./weights/ALE_Breakout-v5_dqn_final.pth\",\n",
    "    env_name=\"ALE/Breakout-v5\",\n",
    "    num_episodes=30,\n",
    "    record_episodes=5,\n",
    "    output_dir='./eval_runs',\n",
    "    device=None,\n",
    "    record_length=18000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
